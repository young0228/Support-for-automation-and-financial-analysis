# -*- coding: utf-8 -*-
"""insurance commission.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UECX9XPDliAKdFthZp6kE7Kzb7u8kWuZ
"""

!pip install pypdf2
!pip install pdfplumber
!pip install pandas
!pip install openpyxl

import re
import pdfplumber
import pandas as pd
import os

from google.colab import drive
drive.mount('/content/drive')

def extract_payment_month(filename, pattern=r'(\d{4}-\d{2})'):
    match = re.search(pattern, filename)
    return match.group(1) if match else 'N/A'

def is_valid_number(s):
    if not s:
        return False
    try:
        float(s.replace(',', '').replace('(', '-').replace(')', ''))
        return True
    except:
        return False

def clean_number(s):
    try:
        s = str(s).strip()
        if '(' in s and ')' in s:
            s = '-' + s.replace('(', '').replace(')', '')
        s = s.replace(',', '').replace('$', '')
        return float(s) if s else 0.0
    except Exception:
        return 0.0

# CGF
def process_cgf(pdf_path):
    current_company = None
    data_rows = []
    filename = os.path.basename(pdf_path)
    payment_month = extract_payment_month(filename)
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            tables = page.extract_tables()
            for table in tables:
                for row in table:
                    # Total Payment amount & Header hanger filtering
                    if not row or not row[0]:
                        continue
                    if any(x in str(row[0]).lower() for x in ['total', 'withholding', 'payment amount', 'group']):
                        continue
                    # Insurer name detection
                    if any(x in str(row[0]).strip() for x in ['FMHP', 'MCS', 'TRIPLE S', 'PSM']):
                        current_company = row[0].strip()
                        continue
                    # Data row processing
                    if current_company and len(row) >=7:
                        try:
                            data = {
                                'Product Type': 'Health',
                                'Payment Agency': 'CGF',
                                'Insurance Company': current_company,
                                'Insured': (row[0] or '').strip(),
                                'Payment Month': payment_month,
                                'Period': re.search(r'\b\d{6}\b', (row[3] or '')).group(0) if re.search(r'\b\d{6}\b', (row[3] or '')) else '',
                                'Policy Number 1': row[1].strip(),
                                'Policy Number 2': row[2].strip(),
                                'Premium': clean_number(row[4]),
                                'Commission Rate': clean_number(row[5]),
                                'Gross Commission': clean_number(row[6]),
                            }
                            data_rows.append(data)
                        except Exception as e:
                            print(f"CGF Error: {pdf_path}")
                            print(f"Problem row: {row}")
                            print(f"Detailed Error: {str(e)}\n")
    return data_rows

# Delta
def process_delta(pdf_path):
    data_rows = []
    filename = os.path.basename(pdf_path)
    payment_month = extract_payment_month(filename)

    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            tables = page.extract_tables()
            for table_idx, table in enumerate(tables):
                for row_idx, row in enumerate(table):
                    try:
                        # Skip header rows
                        if table_idx == 0 and row_idx == 0:
                            continue
                        if len(row) < 17:
                            continue
                        data = {
                            'Product Type': 'Health',
                            'Payment Agency': 'Delta',
                            'Insurance Company': 'Delta',
                            'Insured': (row[0] or '').strip(),
                            'Payment Month': payment_month,
                            'Period': (row[9] or '').strip(),
                            'Policy Number 1': (row[2] or '').split('-')[0].strip(),  # IDNumber
                            'Policy Number 2': '',
                            'Premium': clean_number(row[5]),
                            'Commission Rate': clean_number(row[15]),
                            'Gross Commission': clean_number(row[6]),
                        }
                        data_rows.append(data)
                    except Exception as e:
                        print(f"Delta Error: {pdf_path}")
                        print(f"Problem row: {row}")
                        print(f"Detailed Error: {str(e)}\n")
    return data_rows

# Jjaramillo
def process_jjaramillo(pdf_path):
    data_rows = []
    filename = os.path.basename(pdf_path)
    payment_month = extract_payment_month(filename, r'(?i)jjaramillo_(\d{4}-\d{2})')

    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            text = page.extract_text()
            lines = [line.strip() for line in text.split('\n') if line.strip()]

            # Find header rows
            header_index = -1
            for idx, line in enumerate(lines):
                if "Customer" in line and "Policy Number" in line:
                    header_index = idx
                    break
            if header_index == -1:
                continue

            # Multiline cell merge
            merged_lines = []
            current_line = ""
            for line in lines[header_index + 1:]:
                # Checking whether the policy number includes (including all numbers sequences)
                if re.search(r'\b(\d+|[A-Z]{2,}\d+)\b', line):
                    if current_line:
                        merged_lines.append(current_line)
                    current_line = line
                else:
                    current_line += " " + line
            if current_line:
                merged_lines.append(current_line)

            # Data row processing
            for line in merged_lines:
                parts = line.split()
                try:
                    # Find the location of a policy number
                    policy_idx = next(i for i, part in enumerate(parts)
                                    if re.match(r'^(\d+|[A-Z]{2,}\d+)', part))

                    # Find Eff Date Locations
                    date_idx = -1
                    for i in range(policy_idx + 1, len(parts)):
                        if re.match(r'\d{1,2}/\d{1,2}/\d{4}', parts[i]):
                            date_idx = i
                            break
                    if date_idx == -1:
                        continue

                    # Recalculate column indexes
                    premium_idx = date_idx + 4  # Agency Gross
                    rate_idx = date_idx + 3     # Rate
                    gross_idx = date_idx + 6    # Pay Balance

                    # Data Extraction
                    data = {
                        'Product Type': 'Property',
                        'Payment Agency': 'J.Jaramillo',
                        'Insurance Company': 'J.Jaramillo',
                        'Insured': ' '.join(parts[:policy_idx]),
                        'Payment Month': payment_month,
                        'Period': parts[date_idx],
                        'Policy Number 1': parts[policy_idx],
                        'Policy Number 2': '',
                        'Premium': clean_number(parts[premium_idx]) if len(parts) > premium_idx else 0,
                        'Commission Rate': clean_number(parts[rate_idx]) if len(parts) > rate_idx else 0,
                        'Gross Commission': clean_number(parts[gross_idx]) if len(parts) > gross_idx else 0,
                    }
                    data_rows.append(data)
                except Exception as e:
                    print(f"JJaramillo Error: {pdf_path}")
                    print(f"Problem row: {line}")
                    print(f"Detailed Error: {str(e)}\n")

    return data_rows

def process_eaia(pdf_path):
    data_rows = []
    filename = os.path.basename(pdf_path)
    payment_month = extract_payment_month(filename, r'EAIA_(\d{4}-\d{2})')

    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            text = page.extract_text()
            lines = [line.strip() for line in text.split('\n') if line.strip()]

            data_start_index = -1
            for idx, line in enumerate(lines):
                if "INSURED NAME" in line:
                    data_start_index = idx + 2
                    break

            if data_start_index == -1:
                print(f"‚ñ∑ EAIA data not found starting point: {filename}")
                continue

            raw_data_lines = lines[data_start_index:]
            merged_lines = []
            i = 0
            while i < len(raw_data_lines):
                line = raw_data_lines[i]

                if any(keyword in line for keyword in ["Total", "Net Balance", "Amount to be withheld"]):
                    i += 1
                    continue

                # Multi -line name merge
                if not re.search(r'\d{2}-\d{2}-\d{2}', line) and len(line.split()) < 3:
                    if i + 2 < len(raw_data_lines):
                        next_line = raw_data_lines[i + 1]
                        if next_line.split()[0].isdigit():
                            second_line = next_line
                            merged_line = f"{line} {second_line}"
                            i += 2
                        else:
                            merged_line = line
                            i += 1
                        merged_lines.append(merged_line)
                        continue

                # General data line
                if re.search(r'\d{2}-\d{2}-\d{2}', line):
                    merged_lines.append(line)
                i += 1

            # Data extraction
            for line in merged_lines:
                parts = line.split()
                if len(parts) < 8:
                    continue

                try:
                    policy_idx = -1
                    for j, part in enumerate(parts):
                        if part.startswith(("UC-", "FMH-", "DDE-", "JM-", "IN-", "AI-")):
                            policy_idx = j
                            break

                    if policy_idx == -1:
                        continue

                    # When dividing 13, 0 ~ 2 times use as Insured
                    if len(parts) == 13:
                        insured_name = ' '.join(parts[:3]).strip()
                    else:
                        first_num_idx = next((j for j, part in enumerate(parts)
                                            if re.match(r'^\d+$', part)), policy_idx)
                        insured_name = ' '.join(parts[:first_num_idx]).strip()

                    date_idx = next((j for j, part in enumerate(parts)
                                   if re.match(r'\d{2}-\d{2}-\d{2}', part)), -1)
                    period = parts[date_idx] if date_idx != -1 else ""
                    policy_number = parts[policy_idx]
                    premium = parts[policy_idx + 2] if policy_idx + 2 < len(parts) else "0"
                    commission = parts[-1]

                    data = {
                        'Product Type': 'Property',
                        'Payment Agency': 'EAIA',
                        'Insurance Company': 'EAIA',
                        'Insured': insured_name,
                        'Payment Month': payment_month,
                        'Period': period,
                        'Policy Number 1': policy_number,
                        'Policy Number 2': '',
                        'Premium': clean_number(premium),
                        'Commission Rate': '',
                        'Gross Commission': clean_number(commission),
                    }
                    data_rows.append(data)

                except Exception as e:
                    print(f"EAIA Error: {pdf_path}")
                    print(f"Problem Row: {line}")
                    print(f"Detailed Error: {str(e)}\n")

    return data_rows

# Universal
def process_universal(pdf_path):
    data_rows = []
    filename = os.path.basename(pdf_path)
    payment_month = extract_payment_month(filename)

    # Column mapping (fixed location)
    COLUMN_MAP = {
        'policy': 0,    # Policy Number
        'eff_date': 1,  # Effective Date
        'insured': 4,   # Insured Name
        'premium': 8,   # Premium
        'rate': 9,      # Comm. Rate
        'commission': 10 # Comm. Amount
    }

    with pdfplumber.open(pdf_path) as pdf:
        for page_num, page in enumerate(pdf.pages, 1):
            print(f"\n‚ñ∑ page {page_num} processing")

            table_settings = {
                "vertical_strategy": "lines",
                "horizontal_strategy": "lines",
                "explicit_vertical_lines": page.curves + page.edges,
                "explicit_horizontal_lines": page.curves + page.edges,
                "snap_tolerance": 10,
                "join_tolerance": 20
            }
            tables = page.extract_tables(table_settings)
            print(f"‚ñ∑ Number of extracted tables: {len(tables)}")
            for table in tables:
                if not table or len(table) < 2:
                    continue

                # Data extraction (from the second row)
                for row in table[1:]:
                    if len(row) < 11:
                        continue
                    if any("producer" in str(cell).lower() for cell in row):
                        continue

                    # Add header filtering
                    if (
                        str(row[COLUMN_MAP['insured']]).strip() == "Insured Name" or
                        str(row[COLUMN_MAP['eff_date']]).strip() == "Effective Date" or
                        str(row[COLUMN_MAP['policy']]).strip() == "Policy Number"
                    ):
                        continue

                    try:
                        data = {
                            'Product Type': 'Property',
                            'Payment Agency': 'Universal Ins',
                            'Insurance Company': 'Universal Ins',
                            'Insured': row[COLUMN_MAP['insured']].replace(',', ''),
                            'Payment Month': payment_month,
                            'Period': row[COLUMN_MAP['eff_date']],
                            'Policy Number 1': row[COLUMN_MAP['policy']],
                            'Policy Number 2': '',
                            'Premium': clean_number(row[COLUMN_MAP['premium']]),
                            'Commission Rate': clean_number(row[COLUMN_MAP['rate']]),
                            'Gross Commission': clean_number(row[COLUMN_MAP['commission']]),
                        }
                        data_rows.append(data)
                    except Exception as e:
                      pass
    return data_rows

def process_mapfre(pdf_path):
    data_rows = []
    filename = os.path.basename(pdf_path)
    payment_month = extract_payment_month(filename)
    is_life = 'life' in filename.lower()

    with pdfplumber.open(pdf_path, laparams={"detect_vertical": True}) as pdf:
        last_page = pdf.pages[-1]
        text = last_page.extract_text()
        lines = [line.strip() for line in text.split('\n') if line.strip()]

        if is_life:
            # Life Processing --------------------------------------------------
            header_idx = next((i for i, line in enumerate(lines)
                             if "Policy Number" in line and "Comm. Type Paid" in line), -1)
            if header_idx == -1:
                print(f"‚ñ∑ Life Header Not Found: {filename}")
                return []

            for line in lines[header_idx+1:]:
                if not re.match(r'^\d', line):
                    continue

                # Direct search for amount/commission rate/fee pattern
                premium_match = re.search(r'\$([\d,]+\.\d{2})', line)
                rate_match = re.search(r'(\d+\.\d{2})%', line)
                commission_match = re.search(r'\$([\d,]+\.\d{2})(?!.*\$)', line)  # Last $ Value

                # Policy Number 1 Extraction (First Digit)
                policy = re.split(r'\s+', line)[0].strip()

                # 1. Remove Policy Number 1
                insured_field = re.sub(r'^\d+\s+', '', line)
                # 2. Policyholder Name, Policy Number 2, Date Pattern Matching
                pattern = re.compile(
                    r'^([A-Za-z& \.,]+?)\s+(\d+)\s+(\d{2}/\d{2}/\d{4})'
                )
                match = pattern.search(insured_field)
                if match:
                    insured_name = match.group(1).strip()
                    policy_number_2 = match.group(2)
                    period = match.group(3)
                else:
                    insured_name = insured_field.split('   ')[0].strip()
                    policy_number_2 = ''
                    period = ''
                insured_name = insured_name.replace(',', '').strip()

                data = {
                    'Product Type': 'Life',
                    'Payment Agency': 'MAPFRE',
                    'Insurance Company': 'MAPFRE',
                    'Insured': insured_name,
                    'Payment Month': payment_month,
                    'Period': period,
                    'Policy Number 1': policy,
                    'Policy Number 2': '',
                    'Premium': clean_number(premium_match.group(0)) if premium_match else 0.0,
                    'Commission Rate': clean_number(rate_match.group(1)) if rate_match else 0.0,
                    'Gross Commission': clean_number(commission_match.group(0)) if commission_match else 0.0,
                }
                data_rows.append(data)

        else:
            # Property Processing -----------------------------------------------
            header_idx = next((i for i, line in enumerate(lines)
                             if "Policy Number" in line and "Commission DI" in line), -1)
            if header_idx == -1:
                print(f"‚ñ∑ Property Header Not Found: {filename}")
                return []

            for line in lines[header_idx+1:]:
                if not re.match(r'^\d', line) or len(line.split()) < 13:
                    continue

                parts = line.split()
                try:
                    data = {
                        'Product Type': 'Property',
                        'Payment Agency': 'MAPFRE',
                        'Insurance Company': 'MAPFRE',
                        'Insured': ' '.join(parts[2:6]).replace(',', ''),
                        'Payment Month': payment_month,
                        'Period': parts[1],
                        'Policy Number 1': parts[0],
                        'Policy Number 2': '',
                        'Premium': clean_number(parts[next(i for i, p in enumerate(parts) if '$' in p)]),
                        'Commission Rate': clean_number(parts[next(i for i, p in enumerate(parts) if '%' in p)].replace('%', '')),
                        'Gross Commission': clean_number(parts[next(i for i, p in enumerate(parts) if i > 10 and '$' in p)]),
                    }
                    data_rows.append(data)
                except Exception as e:
                        print(f"‚ñ∑ MAPFRE Error: {filename}")
                        print(f"Detailed Error: {str(e)}\n")
                        print(f"\n‚ñ∑ Number of final extraction data: {len(data_rows)}")
    return data_rows

def batch_process_data_only(root_folder, data_file_path, template_file_path=None):
    """
    Extract PDF data and save to data-only Excel file
    VBA macros are managed in separate template file
    """
    import os
    import pandas as pd

    all_data = []
    existing_data = []

    # 1. Load existing data file (if exists)
    if os.path.exists(data_file_path):
        try:
            existing_df = pd.read_excel(data_file_path, sheet_name='Main')
            existing_data = existing_df.to_dict('records')
            print(f"‚úì Existing data loaded: {len(existing_data)} rows")
        except:
            print("‚úì Creating new data file")
    else:
        print("‚úì Creating new data file")

    # 2. PDF data extraction

    # CGF
    cgf_folder = os.path.join(root_folder, 'CGF')
    if os.path.exists(cgf_folder):
        for filename in os.listdir(cgf_folder):
            if filename.lower().endswith('.pdf'):
                pdf_path = os.path.join(cgf_folder, filename)
                print(f"‚ñ∑ CGF Processing: {filename}")
                cgf_data = process_cgf(pdf_path)
                print(f"‚ñ∑ CGF Number of data: {len(cgf_data)}")
                all_data.extend(cgf_data)

    # Delta
    delta_folder = os.path.join(root_folder, 'Delta')
    if os.path.exists(delta_folder):
        for filename in os.listdir(delta_folder):
            if filename.lower().endswith('.pdf'):
                pdf_path = os.path.join(delta_folder, filename)
                print(f"‚ñ∑ Delta Processing: {filename}")
                delta_data = process_delta(pdf_path)
                print(f"‚ñ∑ Delta Number of data: {len(delta_data)}")
                all_data.extend(delta_data)

    # EAIA
    eaia_folder = os.path.join(root_folder, 'EAIA')
    if os.path.exists(eaia_folder):
        for filename in os.listdir(eaia_folder):
            if filename.lower().endswith('.pdf'):
                pdf_path = os.path.join(eaia_folder, filename)
                print(f"‚ñ∑ EAIA Processing: {filename}")
                eaia_data = process_eaia(pdf_path)
                print(f"‚ñ∑ EAIA Number of data: {len(eaia_data)}")
                all_data.extend(eaia_data)

    # J.Jaramillo
    jjaramillo_folder = os.path.join(root_folder, 'JJaramillo')
    if os.path.exists(jjaramillo_folder):
        for filename in os.listdir(jjaramillo_folder):
            if filename.lower().endswith('.pdf'):
                pdf_path = os.path.join(jjaramillo_folder, filename)
                print(f"‚ñ∑ J.Jaramillo Processing: {filename}")
                jjaramillo_data = process_jjaramillo(pdf_path)
                print(f"‚ñ∑ J.Jaramillo Number of data: {len(jjaramillo_data)}")
                all_data.extend(jjaramillo_data)

    # Universal
    universal_folder = os.path.join(root_folder, 'Universal')
    if os.path.exists(universal_folder):
        for filename in os.listdir(universal_folder):
            if filename.lower().endswith('.pdf'):
                pdf_path = os.path.join(universal_folder, filename)
                print(f"‚ñ∑ Universal Processing: {filename}")
                universal_data = process_universal(pdf_path)
                print(f"‚ñ∑ Universal Number of data: {len(universal_data)}")
                all_data.extend(universal_data)

    # MAPFRE
    mapfre_folder = os.path.join(root_folder, 'MAPFRE')
    if os.path.exists(mapfre_folder):
        for filename in os.listdir(mapfre_folder):
            if filename.lower().endswith('.pdf'):
                pdf_path = os.path.join(mapfre_folder, filename)
                print(f"‚ñ∑ MAPFRE Processing: {filename}")
                mapfre_data = process_mapfre(pdf_path)
                print(f"‚ñ∑ MAPFRE Number of data: {len(mapfre_data)}")
                all_data.extend(mapfre_data)

    # 3. Merge data and remove duplicates
    if all_data:
        combined_data = existing_data + all_data
        df = pd.DataFrame(combined_data)

        # Remove duplicates
        df_dedup = df.drop_duplicates(
            subset=["Policy Number 1", "Period", "Insured"], keep="last"
        )

        print(f"‚úì New rows: {len(all_data)}")
        print(f"‚úì Combined rows: {len(combined_data)}")
        print(f"‚úì After de-duplication: {len(df_dedup)}")

        # Organize column order
        df_final = df_dedup[
            [
                "Product Type",
                "Payment Agency",
                "Insurance Company",
                "Insured",
                "Payment Month",
                "Period",
                "Policy Number 1",
                "Policy Number 2",
                "Premium",
                "Commission Rate",
                "Gross Commission",
            ]
        ]

        # 4. Save to data-only file (no macros)
        with pd.ExcelWriter(data_file_path, engine="openpyxl") as writer:
            df_final.to_excel(writer, sheet_name="Main", index=False)

        print(f"‚úì Data file updated: {data_file_path}")
        print("\n" + "="*60)
        print("üìå NEXT STEPS - Manual Process Required:")
        print("="*60)

        if template_file_path:
            print(f"1. Open template file: {os.path.basename(template_file_path)}")
        else:
            print("1. Open your VBA macro template file (.xlsm)")

        print(f"2. Open data file: {os.path.basename(data_file_path)}")
        print("3. Copy all data from data file's 'Main' sheet")
        print("4. Paste into template file's 'Main' sheet")
        print("5. Click 'Generate All Reports' button in template")
        print("6. Save the completed file with your desired name")
        print("="*60)

        return True

    print("‚ö†Ô∏è No new data found.")
    return False

batch_process_data_only(
    '/content/drive/MyDrive/upwork/Commission Revenue Management Tool Development/2025/PDF',
    '/content/drive/MyDrive/upwork/Commission Revenue Management Tool Development/2025/2025.xlsx',
)